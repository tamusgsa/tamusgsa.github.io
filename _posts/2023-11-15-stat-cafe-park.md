---
title: "Stat Cafe - Jiyoung Park"
excerpt: "Minimum Norm Interpolation by Two-Layer ReLU Neural Networks"
layout: single
classes: wide
layouts_gallery:
  - url: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5066.jpg
    image_path: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5066.jpg
    alt: "IMG_5066.jpg"
  - url: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5027.jpg
    image_path: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5027.jpg
    alt: "IMG_5027.jpg"
  - url: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5033.jpg
    image_path: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5033.jpg
    alt: "IMG_5033.jpg"
  - url: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5044.jpg
    image_path: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5044.jpg
    alt: "IMG_5044.jpg"
  - url: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5056.jpg
    image_path: /assets/images/stat_cafe/Park_Nov_15_2023/IMG_5056.jpg
    alt: "IMG_5056.jpg"
---

- Time: Wednesday 11/15/2023 from 11:30 AM to 12:20 PM
- Location: BLOC 448
- Pizza and drinks provided
- [Presentation]({{ "/assets/files/stat_cafe/Park_Nov_15_2023/StatCafe_Jiyoung_slides.pdf" | relative_url }})
- [Recording](https://www.youtube.com/watch?v=2qdsazfSgE4)

### Topic
Minimum Norm Interpolation by Two-Layer ReLU Neural Networks

### Abstract
Despite their empirical success, theoretical explanations for neural networks' effectiveness remain elusive. One question is how well a neural network fits data and how it interpolates the region without data. We investigate this problem in the case of Two-Layer ReLU neural networks. Our analysis verifies that weight decay penalized empirical risk minimizers converge to a minimum norm interpolant with respect to the dataset size when the strength of the regularizer and the number of parameters are scaled appropriately. As an application of our theoretical findings, we show our results can be leveraged to explore the implicit bias inherent in common optimization algorithms. The talk will primarily introduce the essential theoretical components needed to analyze the Two-Layer ReLU neural network and convergence results.


### Gallery

{% include gallery id="layouts_gallery" %}
