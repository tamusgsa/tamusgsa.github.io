---
title: "Stat Cafe - Raymond Wong"
excerpt: "Balancing Weights for Offline Reinforcement Learning"
layout: single
classes: wide
category: 
  - Stat Cafe
layouts_gallery:
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2011.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2011.JPG
    alt: "IMG_2011.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2016.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2016.JPG
    alt: "IMG_2016.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2017.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2017.JPG
    alt: "IMG_2017.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2018.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2018.JPG
    alt: "IMG_2018.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2019.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2019.JPG
    alt: "IMG_2019.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2022.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2022.JPG
    alt: "IMG_2022.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2028.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2028.JPG
    alt: "IMG_2028.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2029.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2029.JPG
    alt: "IMG_2029.JPG"
  - url: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2032.JPG
    image_path: /assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2032.JPG
    alt: "IMG_2032.JPG"
---


<img src="https://github.com/tamusgsa/tamusgsa.github.io/blob/master/assets/images/stat_cafe/Wong_Feb_17_2025/IMG_2010.JPG?raw=true" alt="Header" width="315" style="float: right;"/> 


###  Balancing Weights for Offline Reinforcement Learning

- Time: Monday 2/17/2025 from 11:30 AM to 12:30 PM
- Location: BLOC 457

[RSVP Link](<https://urldefense.com/v3/__https://forms.gle/JxDsQhxJXQuZniSF6__;!!KwNVnqRv!BsvW55hWCrwJv5WzUYowU9LA5SOQN9iTxVP6ZV97lrKLKlSZCEP5JWAOQyf4Is6EVmRAYDFIEg_Q4GEG0zFaew$>)

### Description
Offline policy evaluation is considered a fundamental and challenging problem in reinforcement learning. In this talk, I will focus on the value estimation of a target policy based on pre-collected data generated from a possibly different policy, under the framework of infinite-horizon Markov decision processes. I will discuss a novel estimator with approximately projected state-action balancing weights for the policy value estimation. These weights are motivated by the marginal importance sampling method in reinforcement learning and the covariate balancing idea in causal inference. Corresponding asymptotic convergence will be presented. Our results scale with both the number of trajectories and the number of decision points at each trajectory. As such, consistency can still be achieved with a limited number of subjects when the number of decision points diverges.

<!--
### Presentation
<iframe src="https://drive.google.com/file/d/1tN9MfS-UIcedYkMafjpg1VxsRcSM0t8T/preview" width="640" height="480" allow="autoplay"></iframe>
-->

<!--
### Recording
<iframe width="560" height="315" src="https://www.youtube.com/embed/YjR7OlZPy2I?si=fbJmXI60nApV2h8H" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
-->

### Gallery (Photos taken by Samantha Williams)

{% include gallery id="layouts_gallery" %}

